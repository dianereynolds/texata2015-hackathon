{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loader\n",
    "\n",
    "This script is for loading data into Elasticsearch for easy exploitation in the rest of the project. All datasets are normalized into a forum-like format, so all texts are either post openers or comments to that post. This will make the task to follow easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from texata import dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CDETS data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read all the XMLs into a Elasticsearch index. Description of the defect is saved as the main text, comments as \"replies\" to that text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sample data\n",
    "#dataloader.CDETSloadES('../data/CDETS-Data-Sample')\n",
    "# Real data\n",
    "dataloader.CDETSloadES('../data/Hackathon-Texata-2015/Defects-ASR9k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cisco Support Forums data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the post files into all of its fields. I save all the data I can find, but in particular the opening text as the main text, and all the replies following it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sample data\n",
    "#dataloader.forumsloadES('../data/Support-Forums-Sample')\n",
    "# Real data\n",
    "dataloader.forumsloadES('../data/Hackathon-Texata-2015/SupportCommunity/RS/content')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Techzone data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each file is badly formed, as it contains several XML trees per file. I have splitted the datafiles into proper separate XML file trees using the tzsplit.sh script I developed, and now I can read it correctly into Elasticsearch. Since these data do not include any kind of replies or the kind, we store each text as an individual \"post\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sample data\n",
    "#dataloader.techzoneESloader('../data/techzone-sample')\n",
    "# Real data\n",
    "dataloader.techzoneESloader('../data/Hackathon-Texata-2015/TechZone-Splitted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stackoverflow data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a public dataset made up of the dump of several stackexchange Q&A communities. While not directly related to CISCO products or services, it does represent a significan corpus of texts of techy nature, and so can be a good training set for a language model in this project.\n",
    "\n",
    "I have downloaded dumps from the following communities:\n",
    "* DSP (Signal Processing)\n",
    "* Network Engineering\n",
    "* Reverse Engineering\n",
    "* Robotics\n",
    "* Security\n",
    "* Serverfault (sistems administration)\n",
    "* Stackoverflow (programming, general tech topics)\n",
    "* Superuser\n",
    "* Unix\n",
    "\n",
    "I will now load all the dumps into Elasticsearch for further reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataloader.stackoverflowESLoader('/media/alvaro/DATA/Datasets/StackOverflow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything should be loaded now, so let's do a little check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch([{'host': 'localhost', 'port': 9200}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to get 100 opener texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(es.search(body={\"fields\": \"text\"}, size=\"100\")['hits']['hits'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to get 100 comments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(es.search(body={\"fields\": \"text.comment\", \"size\":100}, size=\"100\")['hits']['hits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print es.search(fields = \"text\", size=\"100\", index = \"robotics.stackexchange.com\")['hits']['hits']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
